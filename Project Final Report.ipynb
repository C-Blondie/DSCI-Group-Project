{
 "cells": [],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}

# Load the required library
library(tidymodels)

# Load the dataset
data <- read.csv("players.csv")

# Select relevant columns and ensure no missing values
data <- data %>%
  select(experience, age, played_hours) %>%
  drop_na()

# Convert the response variable to a factor
data <- data %>%
  mutate(experience = as.factor(experience))

# Create a data split (80% training, 20% testing)
set.seed(123)
data_split <- initial_split(data, prop = 0.8, strata = experience)
train_data <- training(data_split)
test_data <- testing(data_split)

# Define the k-NN model specification
knn_spec <- nearest_neighbor(weight_func = "rectangular", neighbors = tune()) %>%
  set_engine("kknn") %>%
  set_mode("classification")

# Create a recipe for preprocessing
knn_recipe <- recipe(experience ~ age + played_hours, data = train_data) %>%
  step_normalize(all_predictors())

# Create a workflow
knn_workflow <- workflow() %>%
  add_model(knn_spec) %>%
  add_recipe(knn_recipe)

# Set up cross-validation
cv_folds <- vfold_cv(train_data, v = 5, strata = experience)

# Tune the model to find the best k
set.seed(123)
knn_tune_results <- tune_grid(
  knn_workflow,
  resamples = cv_folds,
  grid = tibble(neighbors = seq(1, 20, 2)), # Test odd k values from 1 to 20
  metrics = metric_set(accuracy)
)

# Extract the best k based on accuracy
best_k <- knn_tune_results %>%
  select_best(metric = "accuracy")

# Finalize the workflow with the best k
final_knn_workflow <- finalize_workflow(knn_workflow, best_k)

# Fit the final model on the training set
final_knn_fit <- fit(final_knn_workflow, data = train_data)

# Evaluate the final model on the testing set
test_results <- predict(final_knn_fit, test_data) %>%
  bind_cols(test_data) %>%
  metrics(truth = experience, estimate = .pred_class)

# Display the best k and evaluation results
cat("Best k:", best_k$neighbors, "\n")
print(test_results)
